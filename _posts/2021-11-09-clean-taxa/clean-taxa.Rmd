---
title: "Cleaning up taxonomic names in subsets of the ANSP volumes"
description: |
  Correcting errors in spelling and clarifying punctuation.
author:
  - name: Amanda Whitmire
    url: https://amandawhitmire.github.io/
    affiliation: Stanford Libraries & Hopkins Marine Station
date: 11-09-2021
collections:
  posts:
    disqus: clean-taxa
    share: [twitter, facebook]
output:
  distill::distill_article:
    self_contained: false
    
draft: false
preview: https://raw.githubusercontent.com/amandawhitmire/blog/main/images/Tierleben.jpg
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Background

Honestly I do not have the time for a whole spiel, but I've come to realize that the variety and volume of OCR errors in transcribing the taxonomic names in the ANSP corpus has negative consequences for the NLP workflow further down the line. In today's adventure, we'll find and correct as many taxonomic names as we can (and probably ignore the rest. Pragmatism wins the day!)

## Workspace setup

Load the Libraries we need.

```{r libraries, echo=TRUE, message=FALSE}
library(tidyverse)
library(rmarkdown)
library(knitr)
library(distill)
library(rvest)
library(dplyr)
```

## Clean the text files because R can get caught up on symbols and things.

```{r echo=TRUE, eval=FALSE}
# creates the list of all the txt files in the directory
listtxt <- list.files("/Users/thalassa/Rcode/blog/data/animals",  # ANSP text files
                       pattern = "*.txt", full.names = T, recursive = FALSE)
outlist <- gsub(".txt", "-clean.txt", listtxt) # change the file name for the cleaned text files

for (k in 1:length(listtxt)){ # for each text file in the directory specified above
  txt <- readLines(listtxt[[k]]) # Load the text file itself
  txtcln <- gsub("[^[:alnum:][:space:]'.]", "", txt) # keep only letters, numbers, spaces, apostrophes and periods
  myfile <- outlist[[k]]
  write_lines(txtcln, myfile)
}
```

Wow that ran quickly! 

## Install and configure GNfinder

Recall that you need to have GNfinder installed on your machine to run it.

To run GNfinder^[Mozzherin, Dmitry, Alexander Myltsev, and Harsh Zalavadiya. Gnames/Gnfinder: V0.14.2. Zenodo, 2021. https://doi.org/10.5281/zenodo.5111562.
], see the instructions [here](https://github.com/gnames/gnfinder). I am a Mac user, and I used the Homebrew option.

```{r}
include_graphics("https://raw.githubusercontent.com/amandawhitmire/blog/main/images/gnfinder-install.jpg")
```

## Running GNfinder

Run gnfinder on smaller parts of the Proceedings of the Academy of Natural Sciences of Philadelphia. Reminder: this runs in a `bash` chunk.

```{bash, echo=TRUE, eval=FALSE}
for f in /Users/thalassa/Rcode/blog/data/animals/*-clean.txt
do 
  echo "Processing $f file..."
  gnfinder "$f" --verify > "${f%.txt}-gnf.txt"
done
```

```{bash eval=FALSE, include=FALSE}
for f in /Users/thalassa/Rcode/blog/data/plants/*.txt
do 
  echo "Processing $f file..."
  gnfinder "$f" --verify > "${f%.txt}_gnf.txt"
done
```

```{bash eval=FALSE, include=FALSE}
gnfinder "/Users/thalassa/Rcode/blog/foo.txt" --verify > "/Users/thalassa/Rcode/blog/foo_gnf.txt"
```

Take the list of GNfinder results and the corresponding list of text file with the ANSP content. Use the GNfinder corrected taxonomic name to replace the possibly misspelled or slightly wrong verbatim name from the text. Save the cleaned text files.

```{r eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
# creates the list of all the txt files in the directory
listgnf <- list.files("/Users/thalassa/Rcode/blog/data/gnfinder-parts-clean", #GNfinder output
                       pattern = "*gnf.txt", full.names = T, recursive = FALSE)
listtxt <- list.files("/Users/thalassa/Rcode/blog/data/animals",  # ANSP text files
                       pattern = "*clean.txt", full.names = T, recursive = FALSE)
outlist <- gsub("clean.txt", "-clean-taxa.txt", listtxt) # change the file name for the cleaned text files

for (k in 1:length(listtxt)){ # for each text file in the directory specified above
  dat <- as_tibble(read_csv(listgnf[[k]])) # Load the corresponding GNfinder info
  txt <- readLines(listtxt[[k]]) # Load the text file itself
  
  for (j in 0:length(dat)){  # for each taxonomic name in the GNfinder results
    txt <- gsub(dat[j,2],dat[j,3],txt) # substitute the corrected "Name" for the "Verbatim" text
    myfile <- outlist[[k]]
    write_lines(txt, myfile)
  }
}

rm(listgnf, listtxt, outlist, dat, j, k, myfile) # clean up!
``` 

That's it - there is now a `txt` file with taxonomic names for every volume of the ANSP corpus. For the purposes of pulling identified taxonomic names into a generalized spaCy pipeline across any given section of the corpus, we'll want one YUGE jsonl file with all of the results we just created. We need to merge, clean up gremlins and de-duplicate. Let's do it!

```{r echo=TRUE, eval=FALSE}

#list all the files in the folder
listfile <- as_tibble(list.files("/Users/thalassa/Rcode/blog/data/taxa-verbatim-txt",
                       pattern = ".txt", full.names = T, recursive = FALSE))

data <- as_tibble(read_lines(listfile[[1]][1]))

for (k in 2:length(listfile[[1]])){
  dat <- as_tibble(read_lines(listfile[[1]][k]))
  data <- bind_rows(data, dat)
}

write_csv(data, "/Users/thalassa/Rcode/blog/data/ansp-taxa-clean.txt")

rm(ldf, listfile, dat, k) # clean up!
``` 

