---
title: "Preparing ANSP subsets for spaCy"
description: |
  Let's prepare a dozen ANSP "catalogues" for analysis in spaCy
author:
  - name: Amanda Whitmire
    url: https://amandawhitmire.github.io/
    affiliation: Stanford Libraries & Hopkins Marine Station
date: 08-13-2021
collections:
  posts:
    disqus: preparing-catalogues
    share: [twitter, facebook]
output:
  distill::distill_article:
    self_contained: false
    
draft: false
preview: https://raw.githubusercontent.com/amandawhitmire/blog/main/images/gnfinder-results.jpg
---
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Background

In my [last post](https://amandawhitmire.github.io/blog/posts/2021-08-05-exploring-gnfinder/), I instroduced myself to the [Global Names Architecture](https://globalnames.org/apps/) tool Global Names Finder (GNfinder). In this post, I'm going to use GNfinder again, applying it on a dozen or so sections of the Proceedings of the Academy of Natural Sciences of Philadelphia (ANSP) volumes that have been picked because they are likely to have a lot of taxonomic names in them. After I ID the genus and species names, we'll save the list from each ANSP section as an entity set for use in [spaCy](https://spacy.io/), a natural language processing tool. Within spaCy (which we will run in a Jupyetr Notebook b/c I stil can't make it work in R), we'll run named entity recognition on the species names, and also look for locations and habitats. Will be cool to see what pops out!

## Workspace setup

Load the Libraries we need.

```{r libraries, echo=TRUE, message=FALSE}
library(tidyverse)
library(rmarkdown)
library(knitr)
library(distill)
library(rvest)
library(dplyr)
```

## Install and configure GNfinder

Recall that you need to have GNfinder installed on your machine to run it.

To run GNfinder^[Mozzherin, Dmitry, Alexander Myltsev, and Harsh Zalavadiya. Gnames/Gnfinder: V0.14.2. Zenodo, 2021. https://doi.org/10.5281/zenodo.5111562.
], see the instructions [here](https://github.com/gnames/gnfinder). I am a Mac user, and I used the Homebrew option.

```{r}
include_graphics("https://raw.githubusercontent.com/amandawhitmire/blog/main/images/gnfinder-install.jpg")
```

## Scrape text from ANSP Volumes

Load the CSV file with our URLs in it. These were preselected with the help of Steve Dilliplane at Drexel University. Each URL leacs to an html site with the OCR'd contents of a section of an ANSP volume. You'll see the titles of the sections in the table we import from a CSV. 

```{r echo=TRUE, paged.print=TRUE, layout="l-body-outset"}
sections <- as_tibble(read_csv("/Users/thalassa/Rcode/blog/data/sections.csv"))
# paged_table(sections)
```

Let's do one URL by hand before we launch into the whole set. First we grab the text from the URL. This is about the easiest web scraping there is - almost no `HTML` structure!

```{r echo=TRUE}
url1 <- sections[[1]][1] # grab the first row, first column
sec1 <- read_html(url1) # read the html of the URL

text <- sec1 %>% # pipe the text in the 'p' sections to a character array
html_nodes("p") %>%
html_text()

# What does it look like?
substr(text, start=1, stop=300)
```

Save the output into a text file that we can run in GNfinder.

```{r echo=TRUE}
infilename = str_c("/Users/thalassa/Rcode/blog/data/",sections[[3]][1],".txt")
outfilename = str_c("/Users/thalassa/Rcode/blog/data/",sections[[3]][1],"_gnresults.txt")
write_lines(text, infilename)
```

## Running GNfinder

This runs in a `bash` terminal.

```{bash, echo=TRUE}
gnfinder /Users/thalassa/Rcode/blog/data/07sec30.txt --verify > /Users/thalassa/Rcode/blog/data/07sec30_gnresults.txt
```

Import the output from GNFinder as a tibble & look at it:

```{r echo=TRUE, paged.print=TRUE, layout="l-body-outset"}
dat <- as_tibble(read_csv(outfilename))
paged_table(dat)
```

## Create entity ruler for spaCy based on GNfinder results. 

spaCy entity tulers look like:

{"label": "HABITAT", "pattern": "temperate oceans"}

{"label": "HABITAT", "pattern": "temperate reefs"}

{"label": "HABITAT", "pattern": "temperate seas"}
...

```{r echo=TRUE}
prefix <- str_c("{","\"label\": ","\"TAXA\","," \"pattern\": \"") # this looks ugly, but if we view it ...
cat(prefix) # it's doing what we want.
sec1ents <- str_c(prefix,dat[[2]],"\"}") # put it together 
# cat(sec1ents) # look at it - should be one entity (taxonomic name) per line, in the right format 
```

## Write new entity set out to a text file for use with spaCy.

```{r echo=TRUE}
write_lines(sec1ents, file = '/Users/thalassa/Rcode/blog/data/sec1ents.jsonl')
```

So, ... that's the end of the pipeline for a single file. I'll come back and modify this to run in a loop after I test out spaCy and confirm that the entity set I just created works. Next step, of course, is to resurface my Jupyter Notebook from a previous excursion into Python and spaCy. Stay tuned!

## Update, 23 August: scraping more files

Now let's get this sucker running in batch mode on several files.

```{r echo=TRUE} 
# Loop through URLs and scrape text
for (i in 1:dim(sections)) {
  url <- sections[[1]][i] # grab the first row, first column
  sec <- read_html(url) # read the html of the URL
  text <- sec %>% # pipe the text in the 'p' sections to a character array
  html_nodes("p") %>%
  html_text()
  infilename = str_c("/Users/thalassa/Rcode/blog/data/",sections[[3]][i],".txt")
  write_lines(text, infilename) # save scraped text in a file
}

rm(url, sec, text, infilename)
```

## Run GNfinder in a loop

We scraped the text (yay!), and now we want to feed that text, one file at a time, through the GNfinder tool. GNfinder will run in a Bash code chunk. I don't know how to do this elegantly, so I'm just going to put all of the files I want to run into a folder and point GNfinder at it. If you have a more targeted way to make this work, say by using a list of filenames provided in a text file, please let me know. This feels a bit klugey to me.

```
## Running GNfinder

Run gnfinder on a section of the Proceedings of the Academy of Natural Sciences of Philadelphia. Reminder: this runs in a `bash` chunk.

```{bash, echo=TRUE}
for f in /Users/thalassa/Rcode/blog/data/*.txt
do 
  echo "Processing $f file..."
  gnfinder "$f" --verify > "${f%.txt}_gnf.txt"
done
```

## Create entity lists for use in spaCy NER pipeline

Now we need to read back in the results from GNfinder and use the taxonomic names it found to create an entity file for each one (in jsonl). Recall that we already created the 'prefix' to the pattern that we need in the entity file. 

```{r echo=TRUE}
setwd("/Users/thalassa/Rcode/blog/data/gnfinder-results/")
ldf <- list() # creates a list
listcsv <- dir(pattern = "*.txt") # creates the list of all the csv files in the directory
outlist <- gsub(".txt", ".jsonl",listcsv ) #change the fole extension for the entity files
for (k in 1:length(listcsv)){
  dat <- as_tibble(read_csv(listcsv[[k]]))
  entout <- str_c(prefix,dat[[2]],"\"}") # put it together
  myfile <- str_c("/Users/thalassa/Rcode/blog/data/taxa-",outlist[[k]])
  write_lines(entout, myfile)
}
  
rm(ldf, listcsv, outlist, dat, k, entout, myfile) # clean up!
```

## Conclusion

Now we have the scraped text of 13 subsections of the ANSP corpus, we identified all of the txaxnomic names in each subsection using GNfinder, and we saved those taxa to text files (jsonl) that are formatted for use as spaCy entity sets in the NER pipeline. Now it's time to return to a Jupyter Notebook to run these through! Stay tuned for that ... 

```{r}
include_graphics("https://raw.githubusercontent.com/amandawhitmire/blog/main/images/spacy-taxa-entities.jpg")
```
<aside>
Here is what part of an entity file looks like.
</aside>