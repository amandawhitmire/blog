---
title: "Preparing ANSP subsets for spaCy"
description: |
  Let's prepare a dozen ANSP "catalogues" for analysis in spaCy
author:
  - name: Amanda Whitmire
    url: https://amandawhitmire.github.io/
    affiliation: Stanford Libraries & Hopkins Marine Station
date: 08-13-2021
collections:
  posts:
    disqus: preparing-catalogues
    share: [twitter, facebook]
output:
  distill::distill_article:
    self_contained: false
    
draft: false
preview: https://raw.githubusercontent.com/amandawhitmire/blog/main/images/gnfinder-results.jpg
---
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Background

In my [last post](https://amandawhitmire.github.io/blog/posts/2021-08-05-exploring-gnfinder/), I instroduced myself to the [Global Names Architecture](https://globalnames.org/apps/) tool Global Names Finder (GNfinder). In this post, I'm going to use GNfinder again, applying it on a dozen or so sections of the Proceedings of the Academy of Natural Sciences of Philadelphia (ANSP) volumes that have been picked because they are likely to have a lot of taxonomic names in them. After I ID the genus and species names, we'll save the list from each ANSP section as an entity set for use in [spaCy](https://spacy.io/), a natural language processing tool. Within spaCy (which we will run in a Jupyetr Notebook b/c I stil can't make it work in R), we'll run named entity recognition on the species names, and also look for locations and habitats. Will be cool to see what pops out!

## Workspace setup

Load the Libraries we need.

```{r libraries, echo=TRUE}
library(tidyverse)
library(rmarkdown)
library(knitr)
library(distill)
library(rvest)
library(dplyr)
```

## Install and configure GNfinder

Recall that you need to have GNfinder installed on your machine to run it.

To run GNfinder^[Mozzherin, Dmitry, Alexander Myltsev, and Harsh Zalavadiya. Gnames/Gnfinder: V0.14.2. Zenodo, 2021. https://doi.org/10.5281/zenodo.5111562.
], see the instructions [here](https://github.com/gnames/gnfinder). I am a Mac user, and I used the Homebrew option.

```{r}
include_graphics("https://raw.githubusercontent.com/amandawhitmire/blog/main/images/gnfinder-install.jpg")
```

## Scrape text from ANSP Volumes

Load the CSV file with our URLs in it. These were preselected with the help of Steve Dilliplane at Drexel University. Each URL leacs to an html site with the OCR'd contents of a section of an ANSP volume. You'll see the titles of the sections in the table we import from a CSV. 

```{r echo=TRUE, paged.print=TRUE, layout="l-body-outset"}
sections <- as_tibble(read_csv("/Users/thalassa/Rcode/blog/data/sections.csv"))
paged_table(sections)
```

Let's do one URL by hand before we launch into the whole set. First we grab the text from the URL. This is about the easiest web scraping there is - almost no `HTML` structure!

```{r echo=TRUE}
url1 <- sections[[1]][1] # grab the first row, first column
sec1 <- read_html(url1) # read the html of the URL

text <- sec1 %>% # pipe the text in the 'p' sections to a character array
html_nodes("p") %>%
html_text()

# What does it look like?
substr(text, start=1, stop=300)
```

Save the output into a text file that we can run in GNfinder.

```{r echo=TRUE}
write_lines(text, file = '/Users/thalassa/Rcode/blog/data/sec1.txt')
```

## Running GNfinder

This runs in a `bash` terminal.

```{bash, echo=TRUE}
# Run gnfinder on a section of the Proceedings of the Academy of Natural Sciences of Philadelphia
gnfinder /Users/thalassa/Rcode/blog/data/sec1.txt --verify > /Users/thalassa/Rcode/blog/data/sec1out.txt
```
Import the output from GNFinder as a tibble & look at it:

```{r echo=TRUE, paged.print=TRUE, layout="l-body-outset"}
dat <- as_tibble(read_csv("/Users/thalassa/Rcode/blog/data/sec1out.txt"))
paged_table(dat)
```

## Create entity ruler for spaCy based on GNfinder results. 

spaCy entity tulers look like:

{"label": "HABITAT", "pattern": "temperate oceans"}

{"label": "HABITAT", "pattern": "temperate reefs"}

{"label": "HABITAT", "pattern": "temperate seas"}
...

```{r echo=TRUE}
prefix <- str_c("{","\"label\": ","\"TAXA\","," \"pattern\": \"") # this looks ugly, but if we view it ...
cat(prefix) # it's doing what we want.

sec1ents <- str_c(prefix,dat[[2]],"\"}")
cat(sec1ents) # look at it - should be one entity (taxonomic name) per line, in the right format 
```

## Write new entity set out to a text file for use with spaCy.

```{r echo=TRUE}
write_lines(sec1ents, file = '/Users/thalassa/Rcode/blog/data/sec1ents.jsonl')
```

So, ... that's the end of the pipeline for a single file. I'll come back and modify this to run in a loop after I test out spaCy and confirm that the entity set I just created works. Next step, of course, is to resurface my Jupyetr Notebook from a previous excursion into Python and spaCy. Stay tuned!
