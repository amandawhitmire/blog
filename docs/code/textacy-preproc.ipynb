{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6a4b1-e41c-479b-8dd1-01cda4cc38bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39d950c4-3c09-40a4-9348-621228f43e6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Downloading spaCy models\n",
    "\n",
    "The first step is to download the spaCy model. The model has been pre-trained on annotated English corpora. You only have to run these code cells below the first time you run the notebook; after that, you can skip right to step 2 and carry on from there. (If you run them again later, nothing bad will happen; it’ll just download again.) You can also run spaCy in other notebooks on your computer in the future, and you’ll be able to skip the step of downloading the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "233dfe3d-6b88-44d7-a966-732b24a677bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports the module you need to download and install the spaCy models\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69456ed4-c6bb-4ebc-a39e-4a47795dc377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.1.0/en_core_web_trf-3.1.0.tar.gz\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Read timed out. (read timeout=15)\")': /github-production-release-asset-2e65be/84940268/d8543880-d520-11eb-9825-186ee14ff7f5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211117%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211117T214135Z&X-Amz-Expires=300&X-Amz-Signature=24223a87820fd1b0f407c25d1b37de40ac5a6c6d86eb07e59e37e94d6a0bfa31&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Den_core_web_trf-3.1.0.tar.gz&response-content-type=application%2Foctet-stream\u001b[0m\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Read timed out. (read timeout=15)\")': /github-production-release-asset-2e65be/84940268/d8543880-d520-11eb-9825-186ee14ff7f5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211117%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211117T214135Z&X-Amz-Expires=300&X-Amz-Signature=24223a87820fd1b0f407c25d1b37de40ac5a6c6d86eb07e59e37e94d6a0bfa31&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Den_core_web_trf-3.1.0.tar.gz&response-content-type=application%2Foctet-stream\u001b[0m\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.1.0/en_core_web_trf-3.1.0.tar.gz (460.2 MB)\n",
      "     |████████████████████████████████| 460.2 MB 13 kB/s               \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from en-core-web-trf==3.1.0) (3.1.3)\n",
      "Requirement already satisfied: spacy-transformers<1.1.0,>=1.0.3 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from en-core-web-trf==3.1.0) (1.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.21.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.26.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: jinja2 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: setuptools in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (58.2.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (21.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (4.62.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: torch>=1.5.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.9.1)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.8.3)\n",
      "Requirement already satisfied: transformers<4.10.0,>=3.4.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (4.9.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (2021.10.8)\n",
      "Requirement already satisfied: sacremoses in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.0.46)\n",
      "Requirement already satisfied: filelock in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (3.3.0)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (6.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.1)\n",
      "Requirement already satisfied: joblib in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.1.0)\n",
      "Requirement already satisfied: six in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#Installs the English spaCy model\n",
    "!{sys.executable} -m pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.1.0/en_core_web_trf-3.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24737779-76ac-484e-a147-eba3c32de127",
   "metadata": {},
   "source": [
    "## 2. Importing spaCy and setting up NLP\n",
    "\n",
    "Run the code cell below to import the spaCy module, and create a functions to loads the Englsih model and run the NLP algorithms (includes named-entity recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3da3a9ee-db2e-490b-bb60-2af2d6514658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports spaCy\n",
    "import spacy\n",
    "\n",
    "#Imports the English model\n",
    "import en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2540ae-ed7b-423e-b227-6bc960124ad1",
   "metadata": {},
   "source": [
    "## 3. Importing other modules\n",
    "\n",
    "There’s various other modules that will be useful in this notebook. The code comments explain what each one is for. This code cell imports all of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c6df1c-a55f-4e59-a2fd-ea262df24746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#io is used for opening and writing files\n",
    "import io\n",
    "\n",
    "#glob is used to find all the pathnames matching a specified pattern (here, all text files)\n",
    "import glob\n",
    "\n",
    "#os is used to navigate your folder directories (e.g. change folders to where you files are stored)\n",
    "import os\n",
    "\n",
    "# for handling data frames, etc.\n",
    "import pandas as pd\n",
    "\n",
    "# Import the spaCy visualizer\n",
    "from spacy import displacy\n",
    "\n",
    "# Import the Entityt Ruler for making custom entities\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "import datetime \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639d1c97-8b4a-4e0f-99f5-a1cd0f1ab378",
   "metadata": {},
   "source": [
    "## 4. Diretory setup\n",
    "\n",
    "Assuming you’re running Jupyter Notebook from your computer’s home directory, this code cell gives you the opportunity to change directories, into the directory where you’re keeping your project files. I've put just a few of the ANSP volumes into a folder called `subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943b2e1c-0104-48f7-8848-83a4cfeb954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the file directory here\n",
    "filedirectory = '/Users/thalassa/Rcode/blog/data/'\n",
    "\n",
    "#Change the working directory to the one you just defined\n",
    "os.chdir(filedirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca33f2ba-d3ca-4cec-9837-801266bc1db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/thalassa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/thalassa/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import json\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag_sents\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37db1ba6-af62-4dc1-bbbe-7e57411d0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/thalassa/Rcode/blog/data/animals-clean/44pg145-clean-taxa.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d017be-46fa-4d42-a38a-d7c6c5e987c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenize.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fbdf4-6c14-441e-84b7-12916179b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce8e8ad9-e22c-459f-8c05-69cc5d3ec150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d460195a-5bfc-400e-a742-6ba3902fb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_data(file):\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "    return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51cfee8e-36e0-4e1e-a25a-e57c132a3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tagged_sents(textfile, jsonfile):\n",
    "    taxalist = load_json_data(jsonfile) \n",
    "    text = load_text_data(textfile)\n",
    "    sentences_with_taxa = []\n",
    "    for sen in sent_tokenize(text):\n",
    "        l = word_tokenize(sen)\n",
    "        if len(set(l).intersection(taxalist))>0:\n",
    "            sentences_with_taxa.append(sen)\n",
    "    return (sentences_with_taxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "974a1583-3127-4a32-9f2c-07a8065dc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_with_taxa = create_tagged_sents(\"/Users/thalassa/Rcode/blog/data/animals-clean/44pg145-clean-taxa.txt\", \"/Users/thalassa/Rcode/blog/data/ansp-taxa-clean.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b50c8-b4e1-48a6-bef8-7d29fd3c05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent_with_taxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0271324-4b8a-4bdb-bbaa-e40bd0b343b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.utcnow()\n",
    "sent_out = []\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #Open the infput filename\n",
    "        sent_with_taxa = create_tagged_sents(filename, \"/Users/thalassa/Rcode/blog/data/ansp-taxa-clean.json\")\n",
    "        sent_out = sent_out + sent_with_taxa\n",
    "        \n",
    "end = datetime.datetime.utcnow()\n",
    "print(f\"Finished at {end}, total time {(end-start).seconds / 60.} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6050445e-7f8d-47f2-888e-91a6214c4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences_with_taxa.txt', 'w') as f:\n",
    "    for item in sent_out:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e80f9-d02a-435f-8a97-43c29fb6f901",
   "metadata": {},
   "source": [
    "## Use NLTK to tag parts of speech. End goal is to keep only sentences with a verb (wheat from chaff, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec34e7ff-aadf-4e13-bedb-7cbd4392380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/thalassa/Rcode/blog/data/sentences_with_taxa.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4296d1a8-cc59-498c-a2b8-9beeba4f434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_text = nltk.word_tokenize(text)\n",
    "text_sentence_tokens = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0daddbc-ea80-46a7-b091-e4595ef180ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CATALOGUE OF AMERICAN Testudinata\\nChelonuea serpentina\\nEmysaurus aliorum.', 'Kinosternum PENNSYLVANicuM.', 'Cistuda id.', 'Terrapene odorata et Boscii Menem 1. c. p. 27.', 'Cistttdo odorata Say 1. c. Emys odorata Schw.', 'Cistudo Clausa\\nCistudo Carolina alior.', 'Emys clausa Schw.', 'Emys virgidata ejusd.', 'Terrapene Carolina viaculata et nelulosa Bell Zool.', 'Cistudo Blaiidiugii Holbr.', 'Emys Muhlenbergii Schcepff.', 'Chersine Muhlenhergii Merrem.', 'In the Catalogue of Amphibia in the collection of the British Museum and in that of the Jardin des Plantes the following species of tortoises are mentioned as coming from the United States.', 'In the English Catalogue are described Emys rivulata E. scripta E. Holbrookii E. macrocephala and E. Bennetii.', \"Till' Kinosternum DouMedayii however forms an cxceptiitn.\", \"In Scha'pff Testudo tricarinata a young animal of some Kinosternum T. cinerea a young picta  T. scripia a young serrata or reticulata  T. rosliata a young Trionyx.\", \"'J' membranacea which is likewise a young Trionyx T. scabra  the description has been made from an immature specimen which if full grown might have been smooth  T. carinata  T. sulcata and finally T. scpiamosa which is not a Chelonian.\", 'Ohservatiu7i.i on the Vespertilio lejwrhiu.s of Linnceus.', 'Our associate Dr. Woodhouse some time ago gave me for examination a species of Bat found by him in the province of Honluras which is undoubtedly the Noctilio dorsatus of GeofFroy de St. Hilaire the Vespertilio leponnus of Linnanis i. p. 47 although the description of  the illustrious Swede is rather short and imperfect.', 'GO describes and figures it as a Noctilio retaining the Linnean specific name.']\n"
     ]
    }
   ],
   "source": [
    "print (text_sentence_tokens[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8218cad1-02e0-4656-9ec5-976e20b278a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CATALOGUE', 'OF', 'AMERICAN', 'Testudinata', 'Chelonuea', 'serpentina', 'Emysaurus', 'aliorum', '.', 'Kinosternum']\n"
     ]
    }
   ],
   "source": [
    "print(words_text[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5b37e07-2f6c-48cf-8a6e-bc3da3b3d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_text = nltk.pos_tag(words_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b763207-d1af-427d-a885-ddfe8cc447eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CATALOGUE', 'NN'), ('OF', 'IN'), ('AMERICAN', 'NNP'), ('Testudinata', 'NNP'), ('Chelonuea', 'NNP'), ('serpentina', 'VBD'), ('Emysaurus', 'NNP'), ('aliorum', 'NN'), ('.', '.'), ('Kinosternum', 'NNP'), ('PENNSYLVANicuM', 'NNP'), ('.', '.'), ('Cistuda', 'NNP'), ('id', 'NN'), ('.', '.'), ('Terrapene', 'NNP'), ('odorata', 'MD'), ('et', 'VB'), ('Boscii', 'NNP'), ('Menem', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_text[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a4e0927-0beb-48c3-91b0-38a938d83a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_word_tokens = []\n",
    "for sentence_token in text_sentence_tokens:\n",
    "    text_word_tokens.append(word_tokenize(sentence_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc4fdace-f0b3-4567-bb1e-f6751d66cdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['CATALOGUE', 'OF', 'AMERICAN', 'Testudinata', 'Chelonuea', 'serpentina', 'Emysaurus', 'aliorum', '.'], ['Kinosternum', 'PENNSYLVANicuM', '.'], ['Cistuda', 'id', '.'], ['Terrapene', 'odorata', 'et', 'Boscii', 'Menem', '1.', 'c.', 'p.', '27', '.'], ['Cistttdo', 'odorata', 'Say', '1.', 'c.', 'Emys', 'odorata', 'Schw', '.'], ['Cistudo', 'Clausa', 'Cistudo', 'Carolina', 'alior', '.'], ['Emys', 'clausa', 'Schw', '.'], ['Emys', 'virgidata', 'ejusd', '.'], ['Terrapene', 'Carolina', 'viaculata', 'et', 'nelulosa', 'Bell', 'Zool', '.'], ['Cistudo', 'Blaiidiugii', 'Holbr', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(text_word_tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26178ed4-5792-43b9-8abf-ec2985acd43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tagged = pos_tag_sents(text_word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc5e17fc-a14d-40bc-b88e-1a005bd89fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('CATALOGUE', 'NN'), ('OF', 'IN'), ('AMERICAN', 'NNP'), ('Testudinata', 'NNP'), ('Chelonuea', 'NNP'), ('serpentina', 'VBD'), ('Emysaurus', 'NNP'), ('aliorum', 'NN'), ('.', '.')], [('Kinosternum', 'NNP'), ('PENNSYLVANicuM', 'NNP'), ('.', '.')], [('Cistuda', 'NNP'), ('id', 'NN'), ('.', '.')], [('Terrapene', 'NNP'), ('odorata', 'MD'), ('et', 'VB'), ('Boscii', 'NNP'), ('Menem', 'NNP'), ('1.', 'CD'), ('c.', 'NN'), ('p.', 'NN'), ('27', 'CD'), ('.', '.')], [('Cistttdo', 'NNP'), ('odorata', 'NNS'), ('Say', 'NNP'), ('1.', 'CD'), ('c.', 'NN'), ('Emys', 'NNP'), ('odorata', 'NN'), ('Schw', 'NNP'), ('.', '.')], [('Cistudo', 'NNP'), ('Clausa', 'NNP'), ('Cistudo', 'NNP'), ('Carolina', 'NNP'), ('alior', 'NN'), ('.', '.')], [('Emys', 'NNP'), ('clausa', 'NN'), ('Schw', 'NNP'), ('.', '.')], [('Emys', 'NNP'), ('virgidata', 'NN'), ('ejusd', 'NN'), ('.', '.')], [('Terrapene', 'NNP'), ('Carolina', 'NNP'), ('viaculata', 'NN'), ('et', 'NN'), ('nelulosa', 'NN'), ('Bell', 'NNP'), ('Zool', 'NNP'), ('.', '.')], [('Cistudo', 'NNP'), ('Blaiidiugii', 'NNP'), ('Holbr', 'NNP'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "print(text_tagged[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "930f1fa5-94ea-4962-ba69-c61434a2d424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (nltk.help.upenn_tagset('V.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c5ee5ae-d490-4566-abba-eb3f4bd34f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb = list(text_tagged[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11569bfb-05a6-474c-a9d3-f3f7b8597b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('CATALOGUE', 'NN'), ('OF', 'IN'), ('AMERICAN', 'NNP'), ('Testudinata', 'NNP'), ('Chelonuea', 'NNP'), ('serpentina', 'VBD'), ('Emysaurus', 'NNP'), ('aliorum', 'NN'), ('.', '.')], [('Kinosternum', 'NNP'), ('PENNSYLVANicuM', 'NNP'), ('.', '.')], [('Cistuda', 'NNP'), ('id', 'NN'), ('.', '.')], [('Terrapene', 'NNP'), ('odorata', 'MD'), ('et', 'VB'), ('Boscii', 'NNP'), ('Menem', 'NNP'), ('1.', 'CD'), ('c.', 'NN'), ('p.', 'NN'), ('27', 'CD'), ('.', '.')], [('Cistttdo', 'NNP'), ('odorata', 'NNS'), ('Say', 'NNP'), ('1.', 'CD'), ('c.', 'NN'), ('Emys', 'NNP'), ('odorata', 'NN'), ('Schw', 'NNP'), ('.', '.')], [('Cistudo', 'NNP'), ('Clausa', 'NNP'), ('Cistudo', 'NNP'), ('Carolina', 'NNP'), ('alior', 'NN'), ('.', '.')], [('Emys', 'NNP'), ('clausa', 'NN'), ('Schw', 'NNP'), ('.', '.')], [('Emys', 'NNP'), ('virgidata', 'NN'), ('ejusd', 'NN'), ('.', '.')], [('Terrapene', 'NNP'), ('Carolina', 'NNP'), ('viaculata', 'NN'), ('et', 'NN'), ('nelulosa', 'NN'), ('Bell', 'NNP'), ('Zool', 'NNP'), ('.', '.')], [('Cistudo', 'NNP'), ('Blaiidiugii', 'NNP'), ('Holbr', 'NNP'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "print(verb[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e23db-747d-4a24-9fe8-932a1f56c67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0726f0b-7e99-49b1-a688-d06359309bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a99ed-640f-4e93-8733-9380b07b09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets up a function so you can run the English model on texts\n",
    "nlp = en_core_web_trf.load()\n",
    "\n",
    "#add the custom entity set (habitats ans taxonomic names)\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before='ner')\n",
    "\n",
    "# this is a large entity set - it takes a while to load.\n",
    "ruler.from_disk(\"/Users/thalassa/streamlit/streamlit-ansp/data/ansp-clean-patterns.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e10db2e-fcbc-4248-8a86-7c4f8a247dd6",
   "metadata": {},
   "source": [
    "## Run code on a single file to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad18e5-f7a2-4683-a859-f8c504c1873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Frances Naomi Clark was an American ichthyologist born in 1894, and was one of the first woman fishery researchers to receive world-wide recognition. Frances Naomi Clark was an American ichthyologist born in 1894, and was one of the first woman fishery researchers to receive world-wide recognition. Seven Ampelis cedrorum specimens were collected in a meadow near lowland fruit trees. Some habitats we know are in the json file are near large rocks, near river mouths, near the bottom and near the ocean. Some species names are Hemigrapsus affinis, Hemigrapsus crassimanus, Hendersonia alternifoliae and Hendersonia celtifolia.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d377030-0a01-4eca-bf14-062a9146a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for token in doc:\n",
    "    rows.append(\n",
    "        {\n",
    "            'Token': token.text, \n",
    "            'Lemma': token.lemma_,\n",
    "            'POS': token.pos_,\n",
    "            'Tag': token.tag_,\n",
    "            'Dependency': token.dep_,\n",
    "            'Head': token.head,\n",
    "            'Ent Type': token.ent_type_,\n",
    "            'IsAlpha': token.is_alpha,\n",
    "            'IsPunct': token.is_punct,\n",
    "            'IsStop': token.is_stop\n",
    "        }\n",
    "    )   \n",
    "tokes = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62d25de2-e36c-418b-9520-2f75b3e57e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Head</th>\n",
       "      <th>Ent Type</th>\n",
       "      <th>IsAlpha</th>\n",
       "      <th>IsPunct</th>\n",
       "      <th>IsStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frances</td>\n",
       "      <td>Frances</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naomi</td>\n",
       "      <td>Naomi</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clark</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>was</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>American</td>\n",
       "      <td>american</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>NORP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>attr</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>born</td>\n",
       "      <td>bear</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>acl</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>born</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1894</td>\n",
       "      <td>1894</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>pobj</td>\n",
       "      <td>in</td>\n",
       "      <td>DATE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>conj</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>attr</td>\n",
       "      <td>was</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>one</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Token          Lemma    POS  Tag Dependency           Head  \\\n",
       "0         Frances        Frances  PROPN  NNP   compound          Clark   \n",
       "1           Naomi          Naomi  PROPN  NNP   compound          Clark   \n",
       "2           Clark          Clark  PROPN  NNP      nsubj            was   \n",
       "3             was             be    AUX  VBD       ROOT            was   \n",
       "4              an             an    DET   DT        det  ichthyologist   \n",
       "5        American       american    ADJ   JJ       amod  ichthyologist   \n",
       "6   ichthyologist  ichthyologist   NOUN   NN       attr            was   \n",
       "7            born           bear   VERB  VBN        acl  ichthyologist   \n",
       "8              in             in    ADP   IN       prep           born   \n",
       "9            1894           1894    NUM   CD       pobj             in   \n",
       "10              ,              ,  PUNCT    ,      punct            was   \n",
       "11            and            and  CCONJ   CC         cc            was   \n",
       "12            was             be   VERB  VBD       conj            was   \n",
       "13            one            one    NUM   CD       attr            was   \n",
       "14             of             of    ADP   IN       prep            one   \n",
       "\n",
       "    Ent Type  IsAlpha  IsPunct  IsStop  \n",
       "0     PERSON     True    False   False  \n",
       "1     PERSON     True    False   False  \n",
       "2     PERSON     True    False   False  \n",
       "3                True    False    True  \n",
       "4                True    False    True  \n",
       "5       NORP     True    False   False  \n",
       "6                True    False   False  \n",
       "7                True    False   False  \n",
       "8                True    False    True  \n",
       "9       DATE    False    False   False  \n",
       "10              False     True   False  \n",
       "11               True    False    True  \n",
       "12               True    False    True  \n",
       "13  CARDINAL     True    False    True  \n",
       "14               True    False    True  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokes.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40a22d-c772-4af3-847e-f56272793d58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running spaCy\n",
    "\n",
    "This step will run every text file throught the complete spaCy pipeline\n",
    "\n",
    "## Note - this takes a while - do not run this chunk unless you want to see the LOC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52afb90b-9452-4c6d-aac2-78ea46b881db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17669.txt\n"
     ]
    }
   ],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "\n",
    "start = datetime.datetime.utcnow()\n",
    "\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_loc to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_nlp.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                voltext = f.read()\n",
    "                #Do English NLP on the contents of the input file\n",
    "                volner = nlp(voltext)\n",
    "                #For each recognized entity\n",
    "                rows = []\n",
    "                for token in doc:\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            'Token': token.text, \n",
    "                            'Lemma': token.lemma_,\n",
    "                            'POS': token.pos_,\n",
    "                            'Tag': token.tag_,\n",
    "                            'Dependency': token.dep_,\n",
    "                            'Head': token.head,\n",
    "                            'Ent Type': token.ent_type_,\n",
    "                            'IsAlpha': token.is_alpha,\n",
    "                            'IsPunct': token.is_punct,\n",
    "                            'IsStop': token.is_stop\n",
    "                        }\n",
    "                    )   \n",
    "                tokes = pd.DataFrame(rows)\n",
    "                tokes.to_csv(outfilename, sep='\\t', index = False, header=True)\n",
    "                \n",
    "end = datetime.datetime.utcnow()\n",
    "print(f\"Finished at {end}, total time {(end-start).seconds / 60.} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa1a56-5a63-495c-8413-a479b4a4936d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79787a-37e7-44ef-9020-6c2b4dc25ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e9d1d-eef8-416b-b8e7-fa8c526a30a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df4e25-7b65-4e82-aca1-56ad2f59db01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
