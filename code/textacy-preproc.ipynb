{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6a4b1-e41c-479b-8dd1-01cda4cc38bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39d950c4-3c09-40a4-9348-621228f43e6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Downloading spaCy models\n",
    "\n",
    "The first step is to download the spaCy model. The model has been pre-trained on annotated English corpora. You only have to run these code cells below the first time you run the notebook; after that, you can skip right to step 2 and carry on from there. (If you run them again later, nothing bad will happen; it’ll just download again.) You can also run spaCy in other notebooks on your computer in the future, and you’ll be able to skip the step of downloading the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233dfe3d-6b88-44d7-a966-732b24a677bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports the module you need to download and install the spaCy models\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69456ed4-c6bb-4ebc-a39e-4a47795dc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installs the English spaCy model\n",
    "!{sys.executable} -m pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.1.0/en_core_web_trf-3.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24737779-76ac-484e-a147-eba3c32de127",
   "metadata": {},
   "source": [
    "## 2. Importing spaCy and setting up NLP\n",
    "\n",
    "Run the code cell below to import the spaCy module, and create a functions to loads the Englsih model and run the NLP algorithms (includes named-entity recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da3a9ee-db2e-490b-bb60-2af2d6514658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports spaCy\n",
    "import spacy\n",
    "\n",
    "#Imports the English model\n",
    "import en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2540ae-ed7b-423e-b227-6bc960124ad1",
   "metadata": {},
   "source": [
    "## 3. Importing other modules\n",
    "\n",
    "There’s various other modules that will be useful in this notebook. The code comments explain what each one is for. This code cell imports all of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c6df1c-a55f-4e59-a2fd-ea262df24746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#io is used for opening and writing files\n",
    "import io\n",
    "\n",
    "#glob is used to find all the pathnames matching a specified pattern (here, all text files)\n",
    "import glob\n",
    "\n",
    "#os is used to navigate your folder directories (e.g. change folders to where you files are stored)\n",
    "import os\n",
    "\n",
    "# for handling data frames, etc.\n",
    "import pandas as pd\n",
    "\n",
    "# Import the spaCy visualizer\n",
    "from spacy import displacy\n",
    "\n",
    "# Import the Entityt Ruler for making custom entities\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "import datetime \n",
    "\n",
    "# pre-processing pipeline\n",
    "import textacy\n",
    "from textacy import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639d1c97-8b4a-4e0f-99f5-a1cd0f1ab378",
   "metadata": {},
   "source": [
    "## 4. Diretory setup\n",
    "\n",
    "Assuming you’re running Jupyter Notebook from your computer’s home directory, this code cell gives you the opportunity to change directories, into the directory where you’re keeping your project files. I've put just a few of the ANSP volumes into a folder called `subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943b2e1c-0104-48f7-8848-83a4cfeb954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the file directory here\n",
    "filedirectory = '/Users/thalassa/Rcode/blog/data/animals/'\n",
    "\n",
    "#Change the working directory to the one you just defined\n",
    "os.chdir(filedirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242b4578-97d8-4792-b0e0-e2d929f06565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets up a function so you can run the English model on texts\n",
    "nlp = en_core_web_trf.load()\n",
    "\n",
    "#add the custom entity set (habitats ans taxonomic names)\n",
    "#ruler = nlp.add_pipe(\"entity_ruler\", before='ner')\n",
    "\n",
    "# this is a large entity set - it takes a while to load.\n",
    "#ruler.from_disk(\"/Users/thalassa/streamlit/streamlit-ansp/ansp-patterns.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a9f77-b492-44bd-bd6b-61767eb0f430",
   "metadata": {},
   "source": [
    "## Run code on a single file to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fde4341-cfda-42ee-85a1-356e4c3da733",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Frances Naomi Clark was an Amer-ican ichthyologist born in 1894, and was one of the first wom.an fishery researchers to receive world-wide recognition.  Frances Naomi Clark was an American ichthyologist born in 1894, and was one of the first woman fishery researchers to receive world-wide recognition. Seven Ampelis cedrorum specimens were collected in a meadow near lowland fruit trees. Some habitats we know are in the json file are near      large rocks, near river mouths, near the bottom and near the ocean. Some species names are Hemigrapsus affinis, Hemigrapsus crassimanus, Hendersonia alternifoliae and Hendersonia celtifolia.\"\n",
    "       ]\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78198c05-b1e4-4914-afa5-c4f3e2c4119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frances Naomi Clark was an Amer-ican ichthyologist born in 1894, and was one of the first wom.an fishery researchers to receive world-wide recognition.  Frances Naomi Clark was an American ichthyologist born in 1894, and was one of the first woman fishery researchers to receive world-wide recognition. Seven Ampelis cedrorum specimens were collected in a meadow near lowland fruit trees. Some habitats we know are in the json file are near      large rocks, near river mouths, near the bottom and near the ocean. Some species names are Hemigrapsus affinis, Hemigrapsus crassimanus, Hendersonia alternifoliae and Hendersonia celtifolia.']\n"
     ]
    }
   ],
   "source": [
    "# Read Dataset \n",
    "Df = pd.read_csv('New Task.csv', encoding = 'latin-1')\n",
    "# Show Dataset\n",
    "Df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c915764d-8ff6-41d0-8d66-3e99e7d4ddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thalassa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries \n",
    "import unidecode \n",
    "import pandas as pd \n",
    "import re \n",
    "import time \n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords') \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from autocorrect import Speller \n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize \n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec34e7ff-aadf-4e13-bedb-7cbd4392380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\" This function will remove \n",
    "        extra whitespaces from the text\n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after extra whitespaces removed .\n",
    "        \n",
    "    Example:\n",
    "    Input : How   are   you   doing   ?\n",
    "    Output : How are you doing ?     \n",
    "        \n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'\\s+') \n",
    "    Without_whitespace = re.sub(pattern, ' ', text)\n",
    "    # There are some instances where there is no space after '?' & ')', \n",
    "    # So I am replacing these with one space so that It will not consider two words as one token.\n",
    "    text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "    return text\n",
    "\n",
    "def remove_newlines_tabs(text):\n",
    "    \"\"\"\n",
    "    This function will remove all the occurrences of newlines, tabs, and combinations like: \\\\n, \\\\.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of newlines, tabs, \\\\n, \\\\ characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\n",
    "    Output : This is her first day at this place. Please, Be nice to her. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n",
    "    Formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "    return Formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4296d1a8-cc59-498c-a2b8-9beeba4f434d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5g/3v18ylv549z8z19q_rfdm_5c0000gq/T/ipykernel_37725/2542987503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mremove_newlines_tabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/5g/3v18ylv549z8z19q_rfdm_5c0000gq/T/ipykernel_37725/3302666513.py\u001b[0m in \u001b[0;36mremove_newlines_tabs\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mFormatted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'. com'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.com'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mFormatted_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "remove_newlines_tabs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5b37e07-2f6c-48cf-8a6e-bc3da3b3d5ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5g/3v18ylv549z8z19q_rfdm_5c0000gq/T/ipykernel_37725/381682251.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mremove_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/5g/3v18ylv549z8z19q_rfdm_5c0000gq/T/ipykernel_37725/3302666513.py\u001b[0m in \u001b[0;36mremove_whitespace\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m     \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\s+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mWithout_whitespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# There are some instances where there is no space after '?' & ')',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# So I am replacing these with one space so that It will not consider two words as one token.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ansp/lib/python3.9/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "remove_whitespace(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b3da7c0-3cfe-49c1-be46-134d9c0d4355",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preprocessing.make_pipeline(\n",
    "    preprocessing.normalize.whitespace,\n",
    "    preprocessing.normalize.hyphenated_words,\n",
    "    preprocessing.normalize.unicode,\n",
    "    preprocessing.normalize.quotation_marks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b533d1-35f1-4eb9-9aae-2640d5234068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frances Naomi Clark was an Amer-ican ichthyologist born in 1894, and was one of the first wom.an fishery researchers to receive world-wide recognition.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc(\"Frances Naomi Clark was an Amer-ican ichthyologist born in 1894, and was one of the first wom.an fishery researchers to receive world-wide recognition.  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d377030-0a01-4eca-bf14-062a9146a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for token in doc:\n",
    "    rows.append(\n",
    "        {\n",
    "            'Token': token.text, \n",
    "            'Lemma': token.lemma_,\n",
    "            'POS': token.pos_,\n",
    "            'Tag': token.tag_,\n",
    "            'Dependency': token.dep_,\n",
    "            'Head': token.head,\n",
    "            'Ent Type': token.ent_type_,\n",
    "            'IsAlpha': token.is_alpha,\n",
    "            'IsPunct': token.is_punct,\n",
    "            'IsStop': token.is_stop\n",
    "        }\n",
    "    )   \n",
    "tokes = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62d25de2-e36c-418b-9520-2f75b3e57e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Head</th>\n",
       "      <th>Ent Type</th>\n",
       "      <th>IsAlpha</th>\n",
       "      <th>IsPunct</th>\n",
       "      <th>IsStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frances</td>\n",
       "      <td>Frances</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naomi</td>\n",
       "      <td>Naomi</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clark</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>was</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>American</td>\n",
       "      <td>american</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>NORP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>attr</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>born</td>\n",
       "      <td>bear</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>acl</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>born</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1894</td>\n",
       "      <td>1894</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>pobj</td>\n",
       "      <td>in</td>\n",
       "      <td>DATE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>conj</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>attr</td>\n",
       "      <td>was</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>one</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Token          Lemma    POS  Tag Dependency           Head  \\\n",
       "0         Frances        Frances  PROPN  NNP   compound          Clark   \n",
       "1           Naomi          Naomi  PROPN  NNP   compound          Clark   \n",
       "2           Clark          Clark  PROPN  NNP      nsubj            was   \n",
       "3             was             be    AUX  VBD       ROOT            was   \n",
       "4              an             an    DET   DT        det  ichthyologist   \n",
       "5        American       american    ADJ   JJ       amod  ichthyologist   \n",
       "6   ichthyologist  ichthyologist   NOUN   NN       attr            was   \n",
       "7            born           bear   VERB  VBN        acl  ichthyologist   \n",
       "8              in             in    ADP   IN       prep           born   \n",
       "9            1894           1894    NUM   CD       pobj             in   \n",
       "10              ,              ,  PUNCT    ,      punct            was   \n",
       "11            and            and  CCONJ   CC         cc            was   \n",
       "12            was             be   VERB  VBD       conj            was   \n",
       "13            one            one    NUM   CD       attr            was   \n",
       "14             of             of    ADP   IN       prep            one   \n",
       "\n",
       "    Ent Type  IsAlpha  IsPunct  IsStop  \n",
       "0     PERSON     True    False   False  \n",
       "1     PERSON     True    False   False  \n",
       "2     PERSON     True    False   False  \n",
       "3                True    False    True  \n",
       "4                True    False    True  \n",
       "5       NORP     True    False   False  \n",
       "6                True    False   False  \n",
       "7                True    False   False  \n",
       "8                True    False    True  \n",
       "9       DATE    False    False   False  \n",
       "10              False     True   False  \n",
       "11               True    False    True  \n",
       "12               True    False    True  \n",
       "13  CARDINAL     True    False    True  \n",
       "14               True    False    True  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokes.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40a22d-c772-4af3-847e-f56272793d58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running spaCy\n",
    "\n",
    "This step will run every text file throught the complete spaCy pipeline\n",
    "\n",
    "## Note - this takes a while - do not run this chunk unless you want to see the LOC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52afb90b-9452-4c6d-aac2-78ea46b881db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17669.txt\n"
     ]
    }
   ],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "\n",
    "start = datetime.datetime.utcnow()\n",
    "\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_loc to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_nlp.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                voltext = f.read()\n",
    "                #Do English NLP on the contents of the input file\n",
    "                volner = nlp(voltext)\n",
    "                #For each recognized entity\n",
    "                rows = []\n",
    "                for token in doc:\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            'Token': token.text, \n",
    "                            'Lemma': token.lemma_,\n",
    "                            'POS': token.pos_,\n",
    "                            'Tag': token.tag_,\n",
    "                            'Dependency': token.dep_,\n",
    "                            'Head': token.head,\n",
    "                            'Ent Type': token.ent_type_,\n",
    "                            'IsAlpha': token.is_alpha,\n",
    "                            'IsPunct': token.is_punct,\n",
    "                            'IsStop': token.is_stop\n",
    "                        }\n",
    "                    )   \n",
    "                tokes = pd.DataFrame(rows)\n",
    "                tokes.to_csv(outfilename, sep='\\t', index = False, header=True)\n",
    "                \n",
    "end = datetime.datetime.utcnow()\n",
    "print(f\"Finished at {end}, total time {(end-start).seconds / 60.} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa1a56-5a63-495c-8413-a479b4a4936d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79787a-37e7-44ef-9020-6c2b4dc25ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e9d1d-eef8-416b-b8e7-fa8c526a30a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df4e25-7b65-4e82-aca1-56ad2f59db01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
