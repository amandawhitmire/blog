{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6a4b1-e41c-479b-8dd1-01cda4cc38bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "047f3c1c-7ea8-4b85-a535-865e1c897e38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running spaCy on the corpus of the Proceedings of the Academy of Natural Sciences of Philadelphia (ANSP).\n",
    "\n",
    "The setup blocks for this notebook are adapted from portions of The Datasitter's Club, specifically, this Notebook: \n",
    "\n",
    "Skallerup Bessette, Lee and Quinn Quinn. “DSC Multilingual Mystery 2: Beware, Lee and Quinn!”. February 27, 2020. https://datasittersclub.github.io/site/dscm2.html.\n",
    "\n",
    "I am so grateful for the work that Quinn so generously documents and shares openly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d950c4-3c09-40a4-9348-621228f43e6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Downloading spaCy models\n",
    "\n",
    "The first step is to download the spaCy model. The model has been pre-trained on annotated English corpora. You only have to run these code cells below the first time you run the notebook; after that, you can skip right to step 2 and carry on from there. (If you run them again later, nothing bad will happen; it’ll just download again.) You can also run spaCy in other notebooks on your computer in the future, and you’ll be able to skip the step of downloading the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233dfe3d-6b88-44d7-a966-732b24a677bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports the module you need to download and install the spaCy models\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69456ed4-c6bb-4ebc-a39e-4a47795dc377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.1.0/en_core_web_trf-3.1.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.1.0/en_core_web_trf-3.1.0.tar.gz (460.2 MB)\n",
      "     |████████████████████████████████| 460.2 MB 15 kB/s              \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from en-core-web-trf==3.1.0) (3.1.3)\n",
      "Requirement already satisfied: spacy-transformers<1.1.0,>=1.0.3 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from en-core-web-trf==3.1.0) (1.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (58.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.26.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (21.0)\n",
      "Requirement already satisfied: jinja2 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (4.62.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.21.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.8.3)\n",
      "Requirement already satisfied: torch>=1.5.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.9.1)\n",
      "Requirement already satisfied: transformers<4.10.0,>=3.4.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (4.9.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (2021.10.8)\n",
      "Requirement already satisfied: sacremoses in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.0.46)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (3.3.0)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (0.10.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-trf==3.1.0) (2.0.1)\n",
      "Requirement already satisfied: joblib in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.1.0)\n",
      "Requirement already satisfied: six in /Users/thalassa/opt/anaconda3/envs/ansp/lib/python3.9/site-packages (from sacremoses->transformers<4.10.0,>=3.4.0->spacy-transformers<1.1.0,>=1.0.3->en-core-web-trf==3.1.0) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#Installs the English spaCy model\n",
    "!{sys.executable} -m pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.1.0/en_core_web_trf-3.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940ef00-4311-4720-b8cc-ffcac6f814bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24737779-76ac-484e-a147-eba3c32de127",
   "metadata": {},
   "source": [
    "## 2. Importing spaCy and setting up NLP\n",
    "\n",
    "Run the code cell below to import the spaCy module, and create a functions to loads the Englsih model and run the NLP algorithms (includes named-entity recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da3a9ee-db2e-490b-bb60-2af2d6514658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports spaCy\n",
    "import spacy\n",
    "\n",
    "#Imports the English model\n",
    "import en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2540ae-ed7b-423e-b227-6bc960124ad1",
   "metadata": {},
   "source": [
    "## 3. Importing other modules\n",
    "\n",
    "There’s various other modules that will be useful in this notebook. The code comments explain what each one is for. This code cell imports all of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04c6df1c-a55f-4e59-a2fd-ea262df24746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#io is used for opening and writing files\n",
    "import io\n",
    "\n",
    "#glob is used to find all the pathnames matching a specified pattern (here, all text files)\n",
    "import glob\n",
    "\n",
    "#os is used to navigate your folder directories (e.g. change folders to where you files are stored)\n",
    "import os\n",
    "\n",
    "# for handling data frames, etc.\n",
    "import pandas as pd\n",
    "\n",
    "# Import the spaCy visualizer\n",
    "from spacy import displacy\n",
    "\n",
    "# Import the Entityt Ruler for making custom entities\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639d1c97-8b4a-4e0f-99f5-a1cd0f1ab378",
   "metadata": {},
   "source": [
    "## 4. Diretory setup\n",
    "\n",
    "Assuming you’re running Jupyter Notebook from your computer’s home directory, this code cell gives you the opportunity to change directories, into the directory where you’re keeping your project files. I've put just a few of the ANSP volumes into a folder called `subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "943b2e1c-0104-48f7-8848-83a4cfeb954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the file directory here\n",
    "filedirectory = '/Users/thalassa/Rcode/blog/data/txt/'\n",
    "\n",
    "#Change the working directory to the one you just defined\n",
    "os.chdir(filedirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "242b4578-97d8-4792-b0e0-e2d929f06565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.entityruler.EntityRuler at 0x17c3decc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sets up a function so you can run the English model on texts\n",
    "nlp = en_core_web_trf.load()\n",
    "\n",
    "#add the custom entity set (habitats ans taxonomic names)\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before='ner')\n",
    "\n",
    "# this is a large entity set - it takes a while to load.\n",
    "ruler.from_disk(\"/Users/thalassa/streamlit/streamlit-ansp/ansp-patterns.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a9f77-b492-44bd-bd6b-61767eb0f430",
   "metadata": {},
   "source": [
    "## Run code on a single file to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fde4341-cfda-42ee-85a1-356e4c3da733",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Frances Naomi Clark was an American ichthyologist born in 1894, and was one of the first woman fishery researchers to receive world-wide recognition. Frances Naomi Clark was an American ichthyologist born in 1894, and was one of the first woman fishery researchers to receive world-wide recognition. Seven Ampelis cedrorum specimens were collected in a meadow near lowland fruit trees. Some habitats we know are in the json file are near large rocks, near river mouths, near the bottom and near the ocean. Some species names are Hemigrapsus affinis, Hemigrapsus crassimanus, Hendersonia alternifoliae and Hendersonia celtifolia.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d377030-0a01-4eca-bf14-062a9146a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for token in doc:\n",
    "    rows.append(\n",
    "        {\n",
    "            'Token': token.text, \n",
    "            'Lemma': token.lemma_,\n",
    "            'POS': token.pos_,\n",
    "            'Tag': token.tag_,\n",
    "            'Dependency': token.dep_,\n",
    "            'Head': token.head,\n",
    "            'Ent Type': token.ent_type_,\n",
    "            'IsAlpha': token.is_alpha,\n",
    "            'IsPunct': token.is_punct,\n",
    "            'IsStop': token.is_stop\n",
    "        }\n",
    "    )   \n",
    "tokes = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62d25de2-e36c-418b-9520-2f75b3e57e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Head</th>\n",
       "      <th>Ent Type</th>\n",
       "      <th>IsAlpha</th>\n",
       "      <th>IsPunct</th>\n",
       "      <th>IsStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frances</td>\n",
       "      <td>Frances</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naomi</td>\n",
       "      <td>Naomi</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clark</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>was</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>American</td>\n",
       "      <td>american</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>NORP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>attr</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>born</td>\n",
       "      <td>bear</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>acl</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>born</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1894</td>\n",
       "      <td>1894</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>pobj</td>\n",
       "      <td>in</td>\n",
       "      <td>DATE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>conj</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>attr</td>\n",
       "      <td>was</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>one</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Token          Lemma    POS  Tag Dependency           Head  \\\n",
       "0         Frances        Frances  PROPN  NNP   compound          Clark   \n",
       "1           Naomi          Naomi  PROPN  NNP   compound          Clark   \n",
       "2           Clark          Clark  PROPN  NNP      nsubj            was   \n",
       "3             was             be    AUX  VBD       ROOT            was   \n",
       "4              an             an    DET   DT        det  ichthyologist   \n",
       "5        American       american    ADJ   JJ       amod  ichthyologist   \n",
       "6   ichthyologist  ichthyologist   NOUN   NN       attr            was   \n",
       "7            born           bear   VERB  VBN        acl  ichthyologist   \n",
       "8              in             in    ADP   IN       prep           born   \n",
       "9            1894           1894    NUM   CD       pobj             in   \n",
       "10              ,              ,  PUNCT    ,      punct            was   \n",
       "11            and            and  CCONJ   CC         cc            was   \n",
       "12            was             be   VERB  VBD       conj            was   \n",
       "13            one            one    NUM   CD       attr            was   \n",
       "14             of             of    ADP   IN       prep            one   \n",
       "\n",
       "    Ent Type  IsAlpha  IsPunct  IsStop  \n",
       "0     PERSON     True    False   False  \n",
       "1     PERSON     True    False   False  \n",
       "2     PERSON     True    False   False  \n",
       "3                True    False    True  \n",
       "4                True    False    True  \n",
       "5       NORP     True    False   False  \n",
       "6                True    False   False  \n",
       "7                True    False   False  \n",
       "8                True    False    True  \n",
       "9       DATE    False    False   False  \n",
       "10              False     True   False  \n",
       "11               True    False    True  \n",
       "12               True    False    True  \n",
       "13  CARDINAL     True    False    True  \n",
       "14               True    False    True  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokes.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40a22d-c772-4af3-847e-f56272793d58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running spaCy\n",
    "\n",
    "In this first test, we are going to look at how spaCy's default \"Locations\" entity recognizer performs. One goal of this NLP project is to identify species occurrences in the ANSP volumes. A species occurrence requires three data points: a SPECIES, seen at a specific PLACE, at a specific DAY/TIME. I will load a custom entity set for the species names in future steps. \n",
    "\n",
    "## Note - this takes a while - do not run this chunk unless you want to see the LOC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52afb90b-9452-4c6d-aac2-78ea46b881db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17669.txt\n"
     ]
    }
   ],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "\n",
    "start = datetime.datetime.utcnow()\n",
    "\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_loc to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_nlp.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                voltext = f.read()\n",
    "                #Do English NLP on the contents of the input file\n",
    "                volner = nlp(voltext)\n",
    "                #For each recognized entity\n",
    "                rows = []\n",
    "                for token in doc:\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            'Token': token.text, \n",
    "                            'Lemma': token.lemma_,\n",
    "                            'POS': token.pos_,\n",
    "                            'Tag': token.tag_,\n",
    "                            'Dependency': token.dep_,\n",
    "                            'Head': token.head,\n",
    "                            'Ent Type': token.ent_type_,\n",
    "                            'IsAlpha': token.is_alpha,\n",
    "                            'IsPunct': token.is_punct,\n",
    "                            'IsStop': token.is_stop\n",
    "                        }\n",
    "                    )   \n",
    "                tokes = pd.DataFrame(rows)\n",
    "                tokes.to_csv(outfilename, sep='\\t', index = False, header=True)\n",
    "                \n",
    "end = datetime.datetime.utcnow()\n",
    "print(f\"Finished at {end}, total time {(end-start).seconds / 60.} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa1a56-5a63-495c-8413-a479b4a4936d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79787a-37e7-44ef-9020-6c2b4dc25ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e9d1d-eef8-416b-b8e7-fa8c526a30a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df4e25-7b65-4e82-aca1-56ad2f59db01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
