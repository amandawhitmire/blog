{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6a4b1-e41c-479b-8dd1-01cda4cc38bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "047f3c1c-7ea8-4b85-a535-865e1c897e38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running spaCy on the corpus of the Proceedings of the Academy of Natural Sciences of Philadelphia (ANSP).\n",
    "\n",
    "The setup blocks for this notebook are adapted from portions of The Datasitter's Club, specifically, this Notebook: \n",
    "\n",
    "Skallerup Bessette, Lee and Quinn Quinn. “DSC Multilingual Mystery 2: Beware, Lee and Quinn!”. February 27, 2020. https://datasittersclub.github.io/site/dscm2.html.\n",
    "\n",
    "I am so grateful for the work that Quinn so generously documents and shares openly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d950c4-3c09-40a4-9348-621228f43e6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Downloading spaCy models\n",
    "\n",
    "The first step is to download the spaCy model. The model has been pre-trained on annotated English corpora. You only have to run these code cells below the first time you run the notebook; after that, you can skip right to step 2 and carry on from there. (If you run them again later, nothing bad will happen; it’ll just download again.) You can also run spaCy in other notebooks on your computer in the future, and you’ll be able to skip the step of downloading the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233dfe3d-6b88-44d7-a966-732b24a677bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports the module you need to download and install the spaCy models\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69456ed4-c6bb-4ebc-a39e-4a47795dc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installs the English spaCy model\n",
    "!{sys.executable} -m pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.1.0/en_core_web_trf-3.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24737779-76ac-484e-a147-eba3c32de127",
   "metadata": {},
   "source": [
    "## 2. Importing spaCy and setting up NLP\n",
    "\n",
    "Run the code cell below to import the spaCy module, and create a functions to loads the Englsih model and run the NLP algorithms (includes named-entity recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da3a9ee-db2e-490b-bb60-2af2d6514658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports spaCy\n",
    "import spacy\n",
    "\n",
    "#Imports the English model\n",
    "import en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2540ae-ed7b-423e-b227-6bc960124ad1",
   "metadata": {},
   "source": [
    "## 3. Importing other modules\n",
    "\n",
    "There’s various other modules that will be useful in this notebook. The code comments explain what each one is for. This code cell imports all of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c6df1c-a55f-4e59-a2fd-ea262df24746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#io is used for opening and writing files\n",
    "import io\n",
    "\n",
    "#glob is used to find all the pathnames matching a specified pattern (here, all text files)\n",
    "import glob\n",
    "\n",
    "#os is used to navigate your folder directories (e.g. change folders to where you files are stored)\n",
    "import os\n",
    "\n",
    "# for handling data frames, etc.\n",
    "import pandas as pd\n",
    "\n",
    "# Import the spaCy visualizer\n",
    "from spacy import displacy\n",
    "\n",
    "# Import the Entityt Ruler for making custom entities\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "import datetime \n",
    "\n",
    "# pre-processing pipeline\n",
    "import textacy\n",
    "from textacy import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639d1c97-8b4a-4e0f-99f5-a1cd0f1ab378",
   "metadata": {},
   "source": [
    "## 4. Diretory setup\n",
    "\n",
    "Assuming you’re running Jupyter Notebook from your computer’s home directory, this code cell gives you the opportunity to change directories, into the directory where you’re keeping your project files. I've put just a few of the ANSP volumes into a folder called `subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943b2e1c-0104-48f7-8848-83a4cfeb954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the file directory here\n",
    "filedirectory = '/Users/thalassa/Rcode/blog/data/animals/'\n",
    "\n",
    "#Change the working directory to the one you just defined\n",
    "os.chdir(filedirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242b4578-97d8-4792-b0e0-e2d929f06565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.entityruler.EntityRuler at 0x18bf0c700>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sets up a function so you can run the English model on texts\n",
    "nlp = en_core_web_trf.load()\n",
    "\n",
    "#add the custom entity set (habitats ans taxonomic names)\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before='ner')\n",
    "\n",
    "# this is a large entity set - it takes a while to load.\n",
    "ruler.from_disk(\"/Users/thalassa/streamlit/streamlit-ansp/data/ansp-patterns.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a9f77-b492-44bd-bd6b-61767eb0f430",
   "metadata": {},
   "source": [
    "## Run code on a single file to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fde4341-cfda-42ee-85a1-356e4c3da733",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Frances Naomi Clark was an American ichthyologist born in 1894, and was one of the first woman fishery researchers to receive world-wide recognition. Frances Naomi Clark was an American ichthyologist born in 1894, and was one of the first woman fishery researchers to receive world-wide recognition. Seven Ampelis cedrorum specimens were collected in a meadow near lowland fruit trees. Some habitats we know are in the json file are near large rocks, near river mouths, near the bottom and near the ocean. Some species names are Hemigrapsus affinis, Hemigrapsus crassimanus, Hendersonia alternifoliae and Hendersonia celtifolia.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d377030-0a01-4eca-bf14-062a9146a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for token in doc:\n",
    "    rows.append(\n",
    "        {\n",
    "            'Token': token.text, \n",
    "            'Lemma': token.lemma_,\n",
    "            'POS': token.pos_,\n",
    "            'Tag': token.tag_,\n",
    "            'Dependency': token.dep_,\n",
    "            'Head': token.head,\n",
    "            'Ent Type': token.ent_type_,\n",
    "            'IsAlpha': token.is_alpha,\n",
    "            'IsPunct': token.is_punct,\n",
    "            'IsStop': token.is_stop\n",
    "        }\n",
    "    )   \n",
    "tokes = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d25de2-e36c-418b-9520-2f75b3e57e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Dependency</th>\n",
       "      <th>Head</th>\n",
       "      <th>Ent Type</th>\n",
       "      <th>IsAlpha</th>\n",
       "      <th>IsPunct</th>\n",
       "      <th>IsStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frances</td>\n",
       "      <td>Frances</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naomi</td>\n",
       "      <td>Naomi</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clark</td>\n",
       "      <td>Clark</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>was</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>American</td>\n",
       "      <td>american</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>NORP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>attr</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>born</td>\n",
       "      <td>bear</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>acl</td>\n",
       "      <td>ichthyologist</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>born</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1894</td>\n",
       "      <td>1894</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>pobj</td>\n",
       "      <td>in</td>\n",
       "      <td>DATE</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBD</td>\n",
       "      <td>conj</td>\n",
       "      <td>was</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>attr</td>\n",
       "      <td>was</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>one</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Token          Lemma    POS  Tag Dependency           Head  \\\n",
       "0         Frances        Frances  PROPN  NNP   compound          Clark   \n",
       "1           Naomi          Naomi  PROPN  NNP   compound          Clark   \n",
       "2           Clark          Clark  PROPN  NNP      nsubj            was   \n",
       "3             was             be    AUX  VBD       ROOT            was   \n",
       "4              an             an    DET   DT        det  ichthyologist   \n",
       "5        American       american    ADJ   JJ       amod  ichthyologist   \n",
       "6   ichthyologist  ichthyologist   NOUN   NN       attr            was   \n",
       "7            born           bear   VERB  VBN        acl  ichthyologist   \n",
       "8              in             in    ADP   IN       prep           born   \n",
       "9            1894           1894    NUM   CD       pobj             in   \n",
       "10              ,              ,  PUNCT    ,      punct            was   \n",
       "11            and            and  CCONJ   CC         cc            was   \n",
       "12            was             be   VERB  VBD       conj            was   \n",
       "13            one            one    NUM   CD       attr            was   \n",
       "14             of             of    ADP   IN       prep            one   \n",
       "\n",
       "    Ent Type  IsAlpha  IsPunct  IsStop  \n",
       "0     PERSON     True    False   False  \n",
       "1     PERSON     True    False   False  \n",
       "2     PERSON     True    False   False  \n",
       "3                True    False    True  \n",
       "4                True    False    True  \n",
       "5       NORP     True    False   False  \n",
       "6                True    False   False  \n",
       "7                True    False   False  \n",
       "8                True    False    True  \n",
       "9       DATE    False    False   False  \n",
       "10              False     True   False  \n",
       "11               True    False    True  \n",
       "12               True    False    True  \n",
       "13  CARDINAL     True    False    True  \n",
       "14               True    False    True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokes.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40a22d-c772-4af3-847e-f56272793d58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running spaCy\n",
    "\n",
    "This step will run every text file throught the complete spaCy pipeline\n",
    "\n",
    "## Note - this takes a while - do not run this chunk unless you want to see the LOC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "413de6b7-0678-4a83-9d10-5721f20ffdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['48sxx10.txt', '55sxx01.txt', '17sxx02_00_nlp.txt', '51sxx23.txt', '50sxx23.txt', 'animals-sxx33.txt', '64sxx09.txt', '17sxx02.txt', 'animals-sxx27.txt', '34sxx06.txt', '46sxx19.txt', '43sxx13.txt', '43sxx07.txt', '42sxx13.txt', '42sxx12.txt', '43sxx12.txt', '46sxx24.txt', '46sxx18.txt', '47sxx18.txt', 'animals-sxx26.txt', '64sxx08.txt', 'animals-sxx32.txt', '50sxx22.txt', '51sxx22.txt', '23sxx01.txt', '61sxx02.txt', '60sxx02.txt', '49sxx05.txt', '48sxx13.txt', '27sxx21.txt', '27sxx09.txt', '55sxx02.txt', '26sxx09.txt', '50sxx20.txt', '51sxx20.txt', 'animals-sxx24.txt', 'animals-sxx30.txt', 'animals-sxx18.txt', '46sxx26.txt', '42sxx10.txt', '43sxx10.txt', '42sxx04.txt', '43sxx11.txt', '42sxx11.txt', '43sxx05.txt', '31sxx26.txt', '44pg213.txt', 'animals-sxx19.txt', 'animals-sxx25.txt', '51sxx21.txt', '50sxx21.txt', '26sxx08.txt', '55sxx03.txt', '60sxx01.txt', '48sxx12.txt', '61sxx01.txt', '49sxx16.txt', '48sxx02.txt', '48sxx16.txt', '49sxx02.txt', '60sxx05.txt', '55sxx07.txt', '55sxx13.txt', '44pg405.txt', '50sxx19.txt', '23sxx06.txt', '51sxx19.txt', 'animals-sxx09.txt', 'animals-sxx21.txt', 'animals-sxx35.txt', '44pg203.txt', '46sxx23.txt', '72sxx09.txt', '29sxx05.txt', '.DS_Store', '42sxx01.txt', '43sxx15.txt', '73sxx08.txt', '34sxx01.txt', '46sxx22.txt', 'animals-sxx34.txt', 'animals-sxx20.txt', 'animals-sxx08.txt', '51sxx18.txt', '50sxx18.txt', '51sxx24.txt', '50sxx30.txt', '50sxx24.txt', '51sxx30.txt', '55sxx06.txt', '48sxx17.txt', '44pg160.txt', '60sxx04.txt', '48sxx03.txt', '61sxx04.txt', '60sxx06.txt', '49sxx15.txt', '49sxx29.txt', '55sxx10.txt', '55sxx04.txt', '50sxx32.txt', '50sxx26.txt', 'animals-sxx36.txt', 'animals-sxx22.txt', '47sxx20.txt', '46sxx20.txt', '46sxx08.txt', '29sxx06.txt', '43sxx16.txt', '30sxx09.txt', '31sxx08.txt', '42sxx03.txt', '30sxx08.txt', '47sxx09.txt', '46sxx09.txt', '46sxx21.txt', 'animals-sxx23.txt', '23sxx04.txt', '51sxx33.txt', '50sxx27.txt', '55sxx11.txt', '49sxx14.txt', '60sxx07.txt', '48sxx14.txt', '57sxx25.txt', '57sxx19.txt', '25sxx12.txt', '39sxx03.txt', '38sxx03.txt', '67sxx11.txt', '66sxx05.txt', '67sxx05.txt', '53sxx07.txt', '52sxx13.txt', '59sxx04.txt', '58sxx04.txt', '71sxx03.txt', '45sxx15.txt', '44sxx15.txt', '41sxx23.txt', '33sxx01.txt', '41sxx22.txt', '71sxx02.txt', '58sxx05.txt', '58sxx11.txt', '70sxx02.txt', '53sxx06.txt', '52sxx12.txt', '53sxx12.txt', '50sxx41.txt', '66sxx10.txt', '67sxx04.txt', '67sxx10.txt', '66sxx04.txt', '39sxx02.txt', '25sxx07.txt', '57sxx18.txt', '24sxx07.txt', '57sxx30.txt', '12sxx03_00_nlp.txt', '25sxx11.txt', '67sxx06.txt', '52sxx10.txt', '53sxx04.txt', '52sxx04.txt', '53sxx38.txt', '58sxx07.txt', '70sxx14.txt', '71sxx14.txt', '45sxx02.txt', '45sxx16.txt', '44sxx02.txt', '40sxx20.txt', '41sxx20.txt', '33sxx03.txt', '32sxx03.txt', '40sxx08.txt', '40sxx09.txt', '32sxx02.txt', '41sxx09.txt', '45sxx17.txt', '59sxx06.txt', '58sxx12.txt', '70sxx15.txt', '53sxx05.txt', '50sxx42.txt', '44pg338.txt', '67sxx13.txt', '67sxx07.txt', '25sxx10.txt', '25sxx14.txt', '57sxx23.txt', '62sxx09.txt', '39sxx05.txt', '67sxx03.txt', '67sxx17.txt', '66sxx03.txt', '53sxx29.txt', 'animals-sxx42.txt', '52sxx15.txt', '53sxx15.txt', '71sxx11.txt', '59sxx02.txt', '45sxx07.txt', '32sxx06.txt', '33sxx07.txt', '45sxx12.txt', '45sxx06.txt', '70sxx04.txt', '71sxx04.txt', '58sxx03.txt', '53sxx14.txt', '53sxx28.txt', '67sxx16.txt', '66sxx02.txt', '67sxx02.txt', '39sxx04.txt', '62sxx08.txt', '39sxx10.txt', '57sxx22.txt', '25sxx15.txt', '56sxx08.txt', '24sxx03.txt', '57sxx08.txt', '57sxx20.txt', '39sxx06.txt', '67sxx14.txt', 'whole_volumes', '53sxx16.txt', 'animals-sxx41.txt', '53sxx02.txt', '70sxx06.txt', '59sxx01.txt', '58sxx01.txt', '71sxx06.txt', '70sxx12.txt', '44pg288.txt', '69sxx09.txt', '41sxx26.txt', '69sxx08.txt', '07sxx30_00_nlp.txt', '58sxx00.txt', '70sxx13.txt', '52sxx17.txt', '53sxx03.txt', '53sxx17.txt', '67sxx01.txt', '66sxx01.txt', '67sxx15.txt', '39sxx07.txt', '57sxx35.txt', '57sxx21.txt', '57sxx09.txt', '25sxx02.txt', '56sxx09.txt', '57sxx04.txt', '56sxx04.txt', '57sxx10.txt', '44pg131.txt', '62sxx12.txt', '67sxx18.txt', '53sxx32.txt', '45sxx08.txt', '41sxx02.txt', '40sxx16.txt', '32sxx09.txt', '41sxx16.txt', '69sxx04.txt', '41sxx17.txt', '68sxx04.txt', '40sxx17.txt', '44pg291.txt', '45sxx09.txt', '45sxx21.txt', '52sxx27.txt', '62sxx07.txt', '62sxx13.txt', '63sxx07.txt', '57sxx39.txt', '56sxx05.txt', '57sxx11.txt', '57sxx05.txt', '56sxx11.txt', '57sxx13.txt', '56sxx07.txt', '57sxx07.txt', '39sxx09.txt', '62sxx05.txt', '62sxx11.txt', '53sxx25.txt', '53sxx19.txt', '71sxx09.txt', '69sxx12.txt', '40sxx15.txt', '74sxx02.txt', '41sxx28.txt', '40sxx14.txt', '69sxx07.txt', '45sxx22.txt', '52sxx18.txt', '44pg331.txt', '63sxx04.txt', '62sxx10.txt', '27sxx62.txt', '62sxx04.txt', '39sxx08.txt', '44pg133.txt', '57sxx06.txt', '57sxx12.txt', '56sxx06.txt', '57sxx16.txt', '57sxx02.txt', '52sxx08.txt', '53sxx20.txt', '52sxx20.txt', '53sxx34.txt', '37sxx11.txt', '37sxx05.txt', '45sxx26.txt', '40sxx04.txt', '68sxx03.txt', '41sxx04.txt', '40sxx10.txt', '07sxx30.txt', '68sxx02.txt', '36sxx04.txt', 'animals-sxx62.txt', '53sxx21.txt', 'animals.DS_Store.txt', '52sxx09.txt', '62sxx01.txt', '57sxx03.txt', '56sxx03.txt', '24sxx08.txt', '57sxx17.txt', '44pg136.txt', '57sxx01.txt', '57sxx15.txt', '56sxx01.txt', '63sxx03.txt', '67sxx09.txt', '66sxx09.txt', '52sxx23.txt', '53sxx23.txt', '58sxx08.txt', '59sxx08.txt', '37sxx06.txt', '41sxx07.txt', '74sxx05.txt', '41sxx12.txt', '69sxx01.txt', '40sxx06.txt', '41sxx06.txt', '68sxx01.txt', '45sxx24.txt', '45sxx30.txt', '36sxx07.txt', '59sxx09.txt', '53sxx36.txt', '52sxx22.txt', '21sxx01.txt', '67sxx20.txt', '67sxx08.txt', '62sxx02.txt', '63sxx02.txt', '57sxx14.txt', '49sxx19.txt', '48sxx19.txt', '49sxx25.txt', '55sxx08.txt', '27sxx03.txt', '26sxx03.txt', '50sxx16.txt', '51sxx16.txt', 'animals-sxx12.txt', 'animals-sxx06.txt', '44pg387.txt', '47sxx04.txt', '44pg230.txt', '46sxx04.txt', '72sxx12.txt', '73sxx06.txt', '72sxx06.txt', '31sxx11.txt', '30sxx05.txt', '31sxx10.txt', '30sxx04.txt', '19sxx03.txt', '30sxx10.txt', '72sxx07.txt', '73sxx07.txt', '44pg219.txt', '64sxx01.txt', '65sxx01.txt', 'animals-sxx07.txt', 'animals-sxx13.txt', '51sxx17.txt', '51sxx03.txt', '55sxx09.txt', '49sxx30.txt', '48sxx18.txt', '49sxx26.txt', '44pg145.txt', '51sxx15.txt', '50sxx15.txt', 'animals-sxx05.txt', 'animals-sxx11.txt', 'animals-sxx39.txt', '65sxx03.txt', '47sxx13.txt', '46sxx13.txt', '47sxx07.txt', 'animals-sxx190.txt', '72sxx05.txt', '73sxx11.txt', '73sxx05.txt', '30sxx06.txt', '43sxx19.txt', '31sxx07.txt', '31sxx13.txt', '73sxx04.txt', '72sxx10.txt', '47sxx12.txt', '46sxx06.txt', '65sxx02.txt', 'animals-sxx38.txt', 'animals-sxx10.txt', 'animals-sxx04.txt', '50sxx14.txt', '51sxx28.txt', '49sxx33.txt', '12sxx03.txt', '60sxx08.txt', '48sxx23.txt', '49sxx23.txt', '26sxx05.txt', '50sxx04.txt', '51sxx04.txt', '44pg342.txt', 'animals-sxx28.txt', 'animals-sxx14.txt', '46sxx02.txt', '46sxx16.txt', '72sxx14.txt', '43sxx08.txt', '43sxx09.txt', '31sxx16.txt', '72sxx15.txt', '73sxx01.txt', '72sxx01.txt', '47sxx03.txt', '46sxx17.txt', '46sxx03.txt', 'animals-sxx15.txt', 'animals-sxx01.txt', 'animals-sxx29.txt', '65sxx07.txt', '64sxx07.txt', '50sxx39.txt', '44PG357.txt', '50sxx05.txt', '51sxx11.txt', '26sxx10.txt', '26sxx04.txt', '44pg169.txt', '49sxx22.txt', '49sxx20.txt', '48sxx20.txt', '27sxx06.txt', '26sxx06.txt', '50sxx13.txt', '51sxx13.txt', '65sxx05.txt', '64sxx05.txt', 'animals-sxx17.txt', 'animals-sxx03.txt', '47sxx15.txt', '73sxx03.txt', '31sxx14.txt', '30sxx01.txt', '73sxx02.txt', '46sxx14.txt', 'animals-sxx02.txt', 'animals-sxx16.txt', '64sxx10.txt', '51sxx12.txt', '50sxx12.txt', '48sxx09.txt', '44pg156.txt', '44pg142.txt']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(filedirectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52afb90b-9452-4c6d-aac2-78ea46b881db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07sxx30.txt\n",
      "12sxx03.txt\n",
      "17sxx02.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5g/3v18ylv549z8z19q_rfdm_5c0000gq/T/ipykernel_45517/498586501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mvoltext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;31m#Do English NLP on the contents of the input file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mvolner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoltext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0;31m#For each recognized entity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ansp/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    997\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ansp/lib/python3.9/site-packages/spacy_transformers/pipeline_component.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[1;32m    183\u001b[0m         \u001b[0minstall_extensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ansp/lib/python3.9/site-packages/spacy_transformers/pipeline_component.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullTransformerBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlistener\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisteners\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ansp/lib/python3.9/site-packages/thinc/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ansp/lib/python3.9/site-packages/spacy_transformers/layers/transformer_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"logger\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mlog_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordpieces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     align = get_alignment(\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mflat_spans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordpieces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokenizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ansp/lib/python3.9/site-packages/spacy_transformers/align.py\u001b[0m in \u001b[0;36mget_alignment\u001b[0;34m(spans, wordpieces, special_tokens)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Tokens can occur more than once, and we need the alignment of each token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# to its place in the concatenated wordpieces array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mtoken_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0malignment\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mwp_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ansp/lib/python3.9/site-packages/spacy_transformers/align.py\u001b[0m in \u001b[0;36mget_token_positions\u001b[0;34m(spans)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mtoken_positions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_positions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mtoken_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Sort all the files in the directory you specified above, alphabetically.\n",
    "\n",
    "start = datetime.datetime.utcnow()\n",
    "\n",
    "#For each of those files...\n",
    "for filename in sorted(os.listdir(filedirectory)):\n",
    "    #If the filename ends with .txt (i.e. if it's actually a text files)\n",
    "    if filename.endswith('.txt'):\n",
    "        #Write out below the name of the file\n",
    "        print(filename)\n",
    "        #The file name of the output file adds _ner_loc to the end of the file name of the input file\n",
    "        outfilename = filename.replace('.txt', '_00_nlp.txt')\n",
    "        #Open the infput filename\n",
    "        with open(filename, 'r') as f:\n",
    "            #Create and open the output filename\n",
    "            with open(outfilename, 'w') as out:\n",
    "                #Read the contents of the input file\n",
    "                voltext = f.read()\n",
    "                #Do English NLP on the contents of the input file\n",
    "                volner = nlp(voltext)\n",
    "                #For each recognized entity\n",
    "                rows = []\n",
    "                for token in volner:\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            'Token': token.text, \n",
    "                            'Lemma': token.lemma_,\n",
    "                            'POS': token.pos_,\n",
    "                            'Tag': token.tag_,\n",
    "                            'Dependency': token.dep_,\n",
    "                            'Head': token.head,\n",
    "                            'Ent Type': token.ent_type_,\n",
    "                            'IsAlpha': token.is_alpha,\n",
    "                            'IsPunct': token.is_punct,\n",
    "                            'IsStop': token.is_stop\n",
    "                        }\n",
    "                    )   \n",
    "                tokes = pd.DataFrame(rows)\n",
    "                tokes.to_csv(outfilename, sep='\\t', index = False, header=True)\n",
    "                \n",
    "end = datetime.datetime.utcnow()\n",
    "print(f\"Finished at {end}, total time {(end-start).seconds / 60.} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa1a56-5a63-495c-8413-a479b4a4936d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79787a-37e7-44ef-9020-6c2b4dc25ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e9d1d-eef8-416b-b8e7-fa8c526a30a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df4e25-7b65-4e82-aca1-56ad2f59db01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
